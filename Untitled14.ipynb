{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa6/iSs1fGOLfFuvso3wtE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehit0320/Research-assistance-chatbot/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKnnly_HgNZ8"
      },
      "outputs": [],
      "source": [
        "!pip install requests chromadb sentence-transformers pymupdf gradio arxiv python-docx reportlab matplotlib plotly pandas networkx python-dotenv --quiet\n",
        "\n",
        "import os, re, json, requests, fitz, time\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from chromadb.utils import embedding_functions\n",
        "import gradio as gr\n",
        "from docx import Document\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "import arxiv\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter\n",
        "import networkx as nx\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# ------------------------------\n",
        "# API Keys (loaded from environment variables)\n",
        "# ------------------------------\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\", \"\")\n",
        "RAPIDAPI_KEY = os.getenv(\"RAPIDAPI_KEY\", \"\")\n",
        "\n",
        "# Validate API keys are set\n",
        "if not OPENROUTER_API_KEY or not RAPIDAPI_KEY:\n",
        "    print(\"‚ö†Ô∏è WARNING: API keys not found in environment variables!\")\n",
        "    print(\"Please create a .env file with your API keys. See env.example for reference.\")\n",
        "    raise ValueError(\"API keys must be set in environment variables. Create a .env file based on env.example\")\n",
        "\n",
        "# API Hosts (can be overridden via environment variables)\n",
        "AI_DETECTION_HOST = os.getenv(\"AI_DETECTION_HOST\", \"ai-detection4.p.rapidapi.com\")\n",
        "PLAGIARISM_HOST = os.getenv(\"PLAGIARISM_HOST\", \"plagiarism-checker-and-auto-citation-generator-multi-lingual.p.rapidapi.com\")\n",
        "HUMANIZE_HOST = os.getenv(\"HUMANIZE_HOST\", \"humanize-ai-content-paraphrasing-api.p.rapidapi.com\")\n",
        "TEXTGEARS_HOST = os.getenv(\"TEXTGEARS_HOST\", \"textgears-textgears-v1.p.rapidapi.com\")\n",
        "\n",
        "# ------------------------------\n",
        "# Session Storage\n",
        "# ------------------------------\n",
        "session_messages = []\n",
        "session_docs = []\n",
        "current_research_topic = None\n",
        "has_uploaded_pdfs = False\n",
        "\n",
        "# ------------------------------\n",
        "# Quality Check Functions\n",
        "# ------------------------------\n",
        "def detect_ai(text):\n",
        "    \"\"\"Detect AI-generated content score\"\"\"\n",
        "    url = f\"https://{AI_DETECTION_HOST}/v1/ai-detection-rapid-api\"\n",
        "    headers = {\n",
        "        \"x-rapidapi-key\": RAPIDAPI_KEY,\n",
        "        \"x-rapidapi-host\": AI_DETECTION_HOST,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\"text\": text, \"lang\": \"en\"}\n",
        "    try:\n",
        "        res = requests.post(url, headers=headers, json=payload, timeout=30)\n",
        "        res.raise_for_status()\n",
        "        data = res.json()\n",
        "        return float(data.get(\"aiScore\", 0))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå AI Detection Error: {e}\")\n",
        "        return 0\n",
        "\n",
        "def check_plagiarism(text):\n",
        "    \"\"\"Check plagiarism percentage\"\"\"\n",
        "    url = f\"https://{PLAGIARISM_HOST}/plagiarism\"\n",
        "    headers = {\n",
        "        \"x-rapidapi-key\": RAPIDAPI_KEY,\n",
        "        \"x-rapidapi-host\": PLAGIARISM_HOST,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"text\": text,\n",
        "        \"language\": \"en\",\n",
        "        \"includeCitations\": False,\n",
        "        \"scrapeSources\": False\n",
        "    }\n",
        "    try:\n",
        "        res = requests.post(url, headers=headers, json=payload, timeout=30)\n",
        "        res.raise_for_status()\n",
        "        data = res.json()\n",
        "        return float(data.get(\"plagiarismPercentage\", 0))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Plagiarism Check Error: {e}\")\n",
        "        return 0\n",
        "\n",
        "def humanize_text(text):\n",
        "    \"\"\"Humanize AI-generated text\"\"\"\n",
        "    url = f\"https://{HUMANIZE_HOST}/v1/paraphrase?raw=true\"\n",
        "    headers = {\n",
        "        \"x-rapidapi-key\": RAPIDAPI_KEY,\n",
        "        \"x-rapidapi-host\": HUMANIZE_HOST,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    try:\n",
        "        res = requests.post(url, headers=headers, json={\"text\": text}, timeout=30)\n",
        "        res.raise_for_status()\n",
        "        data = res.json()\n",
        "        return data.get(\"humanized\", text).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Humanize Error: {e}\")\n",
        "        return text\n",
        "\n",
        "def grammar_check(text):\n",
        "    \"\"\"Check grammar errors\"\"\"\n",
        "    url = f\"https://{TEXTGEARS_HOST}/grammar\"\n",
        "    headers = {\n",
        "        \"x-rapidapi-key\": RAPIDAPI_KEY,\n",
        "        \"x-rapidapi-host\": TEXTGEARS_HOST\n",
        "    }\n",
        "    payload = {\"text\": text, \"language\": \"en-US\"}\n",
        "    try:\n",
        "        res = requests.post(url, headers=headers, data=payload, timeout=30)\n",
        "        res.raise_for_status()\n",
        "        data = res.json()\n",
        "        errors = data.get(\"response\", {}).get(\"errors\", [])\n",
        "        return errors\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Grammar Check Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def correct_text_with_openrouter(text):\n",
        "    \"\"\"Fix grammar using OpenRouter AI\"\"\"\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://researchmate-ai.com\",\n",
        "        \"X-Title\": \"ResearchMate AI\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": \"mistralai/mistral-7b-instruct:free\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"Fix grammar and spelling errors. Maintain academic tone. Return ONLY the corrected text.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Fix grammar:\\n\\n{text}\"}\n",
        "        ]\n",
        "    }\n",
        "    try:\n",
        "        res = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "        res.raise_for_status()\n",
        "        data = res.json()\n",
        "        return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Grammar Correction Error: {e}\")\n",
        "        return text\n",
        "\n",
        "def process_section_quality(text, section_name, ai_thresh=15, plag_thresh=10, max_humanize_attempts=2):\n",
        "    \"\"\"\n",
        "    Run quality checks on a single section.\n",
        "    If AI/plagiarism is high, attempt humanization up to max_humanize_attempts, then request a paraphrase from the LLM.\n",
        "    Returns processed_text and report dict.\n",
        "    \"\"\"\n",
        "    if not text or len(text.strip()) < 30:\n",
        "        return text, {\"ai_score\": 0, \"plagiarism_score\": 0, \"grammar_issues\": 0, \"status\": \"skipped\"}\n",
        "\n",
        "    # Initial checks\n",
        "    ai_score = detect_ai(text)\n",
        "    plag_score = check_plagiarism(text)\n",
        "    processed_text = text\n",
        "\n",
        "    # If needs humanizing, try multiple strategies\n",
        "    humanize_attempt = 0\n",
        "    if ai_score > ai_thresh or plag_score > plag_thresh:\n",
        "        while humanize_attempt < max_humanize_attempts:\n",
        "            try:\n",
        "                processed_text = humanize_text(processed_text)\n",
        "            except Exception:\n",
        "                pass\n",
        "            time.sleep(0.8)\n",
        "            # Re-check quickly\n",
        "            ai_score = detect_ai(processed_text)\n",
        "            plag_score = check_plagiarism(processed_text)\n",
        "            humanize_attempt += 1\n",
        "            if ai_score <= ai_thresh and plag_score <= plag_thresh:\n",
        "                break\n",
        "\n",
        "        # If still flagged, ask OpenRouter to paraphrase to \"human academic style\"\n",
        "        if ai_score > ai_thresh or plag_score > plag_thresh:\n",
        "            prompt = (\n",
        "                f\"Paraphrase the following section to make it read like authentic, human-written academic prose. \"\n",
        "                f\"Keep the technical meaning exactly the same, keep citations/values, but change wording and sentence rhythm. \"\n",
        "                f\"Return ONLY the paraphrased section.\\n\\nSECTION NAME: {section_name}\\n\\n{text}\"\n",
        "            )\n",
        "            try:\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a skilled academic writer. Paraphrase text to sound human and natural.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "                paraphrased = get_ai_response(messages)\n",
        "                if paraphrased and not paraphrased.startswith(\"[AI ERROR]\"):\n",
        "                    processed_text = paraphrased\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Grammar check and correction if many errors\n",
        "    grammar_errors = grammar_check(processed_text)\n",
        "    if grammar_errors and len(grammar_errors) > 2:\n",
        "        try:\n",
        "            processed_text = correct_text_with_openrouter(processed_text)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Final checks for report\n",
        "    final_ai = detect_ai(processed_text)\n",
        "    final_plag = check_plagiarism(processed_text)\n",
        "    final_grammar = grammar_check(processed_text)\n",
        "\n",
        "    status = \"‚úÖ passed\" if final_ai <= ai_thresh and final_plag <= plag_thresh else \"‚ö†Ô∏è needs review\"\n",
        "    report = {\n",
        "        \"ai_score\": final_ai,\n",
        "        \"plagiarism_score\": final_plag,\n",
        "        \"grammar_issues\": len(final_grammar),\n",
        "        \"status\": status\n",
        "    }\n",
        "    return processed_text, report\n",
        "\n",
        "# ------------------------------\n",
        "# OpenRouter AI\n",
        "# ------------------------------\n",
        "FREE_MODELS = [\n",
        "    \"deepseek/deepseek-r1-0528-qwen3-8b:free\",\n",
        "    \"meta-llama/llama-3.2-3b-instruct:free\",\n",
        "    \"google/gemini-flash-1.5:free\",\n",
        "    \"mistralai/mistral-7b-instruct:free\",\n",
        "    \"nousresearch/hermes-3-llama-3.1-405b:free\"\n",
        "]\n",
        "current_model_index = 0\n",
        "\n",
        "def get_ai_response(messages, max_retries=3):\n",
        "    global current_model_index\n",
        "\n",
        "    models_tried = 0\n",
        "    max_models_to_try = len(FREE_MODELS)\n",
        "\n",
        "    while models_tried < max_models_to_try:\n",
        "        model = FREE_MODELS[current_model_index]\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "                headers = {\n",
        "                    \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "                    \"Content-Type\": \"application/json\",\n",
        "                    \"HTTP-Referer\": \"https://researchmate-ai.com\",\n",
        "                    \"X-Title\": \"ResearchMate AI\"\n",
        "                }\n",
        "                payload = {\"model\": model, \"messages\": messages}\n",
        "\n",
        "                if attempt > 0:\n",
        "                    time.sleep(2 ** attempt)\n",
        "\n",
        "                r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)\n",
        "\n",
        "                if r.status_code == 429:\n",
        "                    current_model_index = (current_model_index + 1) % len(FREE_MODELS)\n",
        "                    models_tried += 1\n",
        "                    break\n",
        "\n",
        "                r.raise_for_status() # Raise an exception for HTTP errors\n",
        "\n",
        "                # Attempt to parse JSON, handle potential empty or non-JSON response\n",
        "                try:\n",
        "                    data = r.json()\n",
        "                except json.JSONDecodeError:\n",
        "                    return f\"[AI ERROR] Invalid JSON response from model '{model}'. Response: {r.text}\"\n",
        "\n",
        "                if \"choices\" in data and len(data[\"choices\"]) > 0:\n",
        "                    return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "                else:\n",
        "                    return f\"[AI ERROR] No choices found in response from model '{model}'. Response: {data}\"\n",
        "\n",
        "            except requests.exceptions.RequestException as req_e:\n",
        "                # Catch specific request exceptions for better error reporting\n",
        "                if attempt == max_retries - 1:\n",
        "                    return f\"[AI ERROR] Request failed for model '{model}' after {max_retries} attempts: {str(req_e)}\"\n",
        "            except Exception as e:\n",
        "                # Catch any other unexpected errors during the process\n",
        "                if attempt == max_retries - 1:\n",
        "                    return f\"[AI ERROR] An unexpected error occurred with model '{model}' after {max_retries} attempts: {str(e)}\"\n",
        "\n",
        "        # If we broke out of the inner loop due to 429, try the next model\n",
        "        if models_tried < max_models_to_try:\n",
        "            current_model_index = (current_model_index + 1) % len(FREE_MODELS)\n",
        "            models_tried += 1\n",
        "\n",
        "    return \"‚ö†Ô∏è All models are currently rate limited or encountering persistent errors. Please wait and try again.\"\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# RAG Setup\n",
        "# ------------------------------\n",
        "try:\n",
        "    chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"./chromadb_session\")\n",
        "embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
        "collection = client.get_or_create_collection(name=\"papers_session\", embedding_function=embedding_fn)\n",
        "\n",
        "def add_texts_to_rag(texts, names, source_type=\"pdf\"):\n",
        "    for text, name in zip(texts, names):\n",
        "        doc_id = re.sub(r'[^\\w\\-\\. ]', '', name)[:200]\n",
        "        try:\n",
        "            collection.add(documents=[text], metadatas=[{\"title\": name, \"source\": source_type}], ids=[doc_id])\n",
        "        except Exception as e:\n",
        "            print(\"Chroma add error:\", e)\n",
        "        session_docs.append({\"title\": name, \"text\": text, \"source\": source_type})\n",
        "\n",
        "def query_rag(user_query, research_topic=None):\n",
        "    global has_uploaded_pdfs, current_research_topic\n",
        "\n",
        "    try:\n",
        "        pdf_docs = [d for d in session_docs if d.get(\"source\") == \"pdf\"]\n",
        "        arxiv_docs = [d for d in session_docs if d.get(\"source\") == \"arxiv\"]\n",
        "\n",
        "        if pdf_docs:\n",
        "            results = collection.query(query_texts=[user_query], n_results=3)\n",
        "            docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
        "            pdf_context = \"\\n\\n\".join([d for d in docs if d]) if docs else \"\"\n",
        "\n",
        "            if not pdf_context:\n",
        "                return \"‚ùå No relevant content found in your uploaded PDFs.\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a research assistant. Answer based on provided PDF content.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"CONTENT:\\n{pdf_context}\\n\\nQUESTION: {user_query}\"}\n",
        "            ]\n",
        "            return get_ai_response(messages)\n",
        "\n",
        "        elif current_research_topic and not pdf_docs:\n",
        "            if not arxiv_docs:\n",
        "                papers = fetch_research_papers(current_research_topic, max_results=5)\n",
        "                if papers:\n",
        "                    abstracts = [p[\"abstract\"] for p in papers if p.get(\"abstract\")]\n",
        "                    titles = [p[\"title\"] for p in papers]\n",
        "                    add_texts_to_rag(abstracts, titles, source_type=\"arxiv\")\n",
        "                else:\n",
        "                    return \"‚ùå Could not fetch arXiv papers.\"\n",
        "\n",
        "            results = collection.query(query_texts=[user_query], n_results=3)\n",
        "            docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
        "            arxiv_context = \"\\n\\n\".join([d for d in docs if d]) if docs else \"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"Research assistant for {current_research_topic}.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"TOPIC: {current_research_topic}\\n\\nCONTEXT:\\n{arxiv_context}\\n\\nQUESTION: {user_query}\"}\n",
        "            ]\n",
        "            return get_ai_response(messages)\n",
        "\n",
        "        else:\n",
        "            return \"‚ùå Please upload PDFs or set a research topic.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"[RAG ERROR] {e}\"\n",
        "\n",
        "# ------------------------------\n",
        "# PDF Utilities\n",
        "# ------------------------------\n",
        "def extract_text_from_pdfs(pdf_files):\n",
        "    texts, names = [], []\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            doc = fitz.open(pdf_file.name)\n",
        "            pdf_text = \"\".join([page.get_text() for page in doc])\n",
        "            texts.append(pdf_text)\n",
        "            names.append(os.path.basename(pdf_file.name))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF: {e}\")\n",
        "    return texts, names\n",
        "\n",
        "# ------------------------------\n",
        "# arXiv Utilities\n",
        "# ------------------------------\n",
        "def fetch_research_papers(topic, max_results=5):\n",
        "    papers = []\n",
        "    try:\n",
        "        search = arxiv.Search(query=topic, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
        "        for result in search.results():\n",
        "            papers.append({\n",
        "                \"title\": result.title,\n",
        "                \"authors\": [a.name for a in result.authors],\n",
        "                \"abstract\": result.summary,\n",
        "                \"pdf_url\": result.pdf_url\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(\"arXiv fetch error:\", e)\n",
        "    return papers\n",
        "\n",
        "# ------------------------------\n",
        "# Chat Integration\n",
        "# ------------------------------\n",
        "def combined_chat(idea_topic, pdf_files, user_input):\n",
        "    global current_research_topic, has_uploaded_pdfs\n",
        "\n",
        "    if not user_input:\n",
        "        return \"‚ùå Please enter a question.\"\n",
        "\n",
        "    if idea_topic:\n",
        "        current_research_topic = idea_topic\n",
        "\n",
        "    if pdf_files and not has_uploaded_pdfs:\n",
        "        pdf_texts, pdf_names = extract_text_from_pdfs(pdf_files)\n",
        "        if pdf_texts:\n",
        "            add_texts_to_rag(pdf_texts, pdf_names, source_type=\"pdf\")\n",
        "            has_uploaded_pdfs = True\n",
        "\n",
        "    session_messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    answer = query_rag(user_input, research_topic=current_research_topic)\n",
        "    session_messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    return answer\n",
        "\n",
        "# ------------------------------\n",
        "# Enhanced Paper Generator with Quality Processing\n",
        "# ------------------------------\n",
        "def parse_paper_sections(paper_text):\n",
        "    \"\"\"\n",
        "    Parse paper into canonical sections. Returns Ordered dict-like (normal dict preserves insertion in Python3.7+).\n",
        "    More robust detection of common headings and fallback to splitting by big blank-lines.\n",
        "    \"\"\"\n",
        "    required = [\"Abstract\", \"Introduction\", \"Literature Review\", \"Methodology\",\n",
        "                \"Results and Discussion\", \"Conclusion\", \"References\"]\n",
        "    sections = {}\n",
        "    # Normalize newlines\n",
        "    text = re.sub(r'\\r\\n?', '\\n', paper_text).strip()\n",
        "\n",
        "    # Define a more robust pattern for section headings\n",
        "    # It looks for common headings at the start of a line, optionally followed by newlines/dashes/colons\n",
        "    # and ensures it's not just part of a sentence.\n",
        "    heading_pattern = re.compile(\n",
        "        r'^\\s*(Abstract|Introduction|Literature Review|Related Work|Background|Methodology|Methods|Materials and Methods|Results|Results and Discussion|Discussion|Conclusion|Conclusions|References)\\s*[:\\n\\r\\-]{0,2}\\s*$',\n",
        "        re.IGNORECASE | re.MULTILINE\n",
        "    )\n",
        "\n",
        "    matches = list(heading_pattern.finditer(text))\n",
        "\n",
        "    if matches:\n",
        "        # Collect sections by heading span\n",
        "        for i, m in enumerate(matches):\n",
        "            # Extract heading, normalize it, and get the content between this heading and the next\n",
        "            head = m.group(1).strip()\n",
        "            start = m.end()\n",
        "            end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
        "            content = text[start:end].strip()\n",
        "\n",
        "            # Normalize headings to canonical names\n",
        "            if head.lower() in [\"related work\", \"background\"]:\n",
        "                head = \"Literature Review\"\n",
        "            if head.lower() in [\"methods\", \"materials and methods\"]:\n",
        "                head = \"Methodology\"\n",
        "            if head.lower() == \"results\":\n",
        "                head = \"Results and Discussion\"\n",
        "            if head.lower() == \"conclusions\":\n",
        "                head = \"Conclusion\"\n",
        "\n",
        "            # Avoid overwriting a potentially pre-filled section with an empty one if a heading repeats\n",
        "            if head not in sections or (head in sections and content):\n",
        "                sections[head] = content\n",
        "    else:\n",
        "        # Fallback: split by double newlines and try to map first few parts\n",
        "        parts = [p.strip() for p in re.split(r'\\n{2,}', text) if p.strip()]\n",
        "        # Heuristics: first part -> Abstract if short, else header\n",
        "        for i, part in enumerate(parts):\n",
        "            if i == 0 and len(part.split()) < 300:\n",
        "                sections.setdefault(\"Abstract\", part)\n",
        "            elif i == 0:\n",
        "                sections.setdefault(\"Introduction\", part)\n",
        "            elif \"method\" in part.lower() or \"experiment\" in part.lower():\n",
        "                sections.setdefault(\"Methodology\", part)\n",
        "            elif \"result\" in part.lower() or \"discussion\" in part.lower():\n",
        "                sections.setdefault(\"Results and Discussion\", part)\n",
        "            elif \"conclusion\" in part.lower():\n",
        "                sections.setdefault(\"Conclusion\", part)\n",
        "            elif \"reference\" in part.lower():\n",
        "                sections.setdefault(\"References\", part)\n",
        "            else:\n",
        "                # distribute uncategorized content to Introduction or Literature Review\n",
        "                if \"Introduction\" not in sections:\n",
        "                    sections.setdefault(\"Introduction\", part)\n",
        "                else:\n",
        "                    sections.setdefault(\"Literature Review\", (sections.get(\"Literature Review\",\"\") + \"\\n\\n\" + part).strip())\n",
        "\n",
        "    # Ensure canonical order and presence of required keys (may be empty)\n",
        "    ordered = {}\n",
        "    for key in [\"Abstract\", \"Introduction\", \"Literature Review\", \"Methodology\", \"Results and Discussion\", \"Conclusion\", \"References\"]:\n",
        "        if key in sections:\n",
        "            ordered[key] = sections[key]\n",
        "        else:\n",
        "            ordered[key] = \"\"  # placeholder empty\n",
        "    return ordered\n",
        "\n",
        "\n",
        "def generate_paper_with_quality_check(topic, enable_quality_check=True, progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    Improved section-wise paper generation:\n",
        "    - Request whole paper with explicit heading format.\n",
        "    - Parse into canonical sections.\n",
        "    - For missing/empty sections, call the LLM to produce that section only (using context).\n",
        "    - Run process_section_quality on each section, rebuild paper, and return files.\n",
        "    \"\"\"\n",
        "    if not topic:\n",
        "        return \"‚ùå Please enter a research topic.\", None, None, \"\"\n",
        "\n",
        "    progress(0, desc=\"Fetching research papers...\")\n",
        "    papers = fetch_research_papers(topic, max_results=5)\n",
        "    abstracts = \" \".join([p[\"abstract\"] for p in papers if p.get(\"abstract\")])\n",
        "\n",
        "    progress(0.15, desc=\"Requesting structured paper from model...\")\n",
        "    # Strong instruction to produce clear labeled sections EXACTLY as required\n",
        "    system_prompt = \"You are an academic writer. Produce a complete research paper. Use the exact headings: Abstract, Introduction, Literature Review, Methodology, Results and Discussion, Conclusion, References. Return sections in that order. Do NOT include any introductory or conversational text before the Abstract or between sections other than the headings. Start directly with 'Abstract'.\"\n",
        "    user_prompt = (\n",
        "        f\"Create a comprehensive research paper on '{topic}' with these sections:\\n\\n\"\n",
        "        \"1. Abstract (150-250 words)\\n2. Introduction\\n3. Literature Review\\n4. Methodology\\n5. Results and Discussion\\n6. Conclusion\\n7. References\\n\\n\"\n",
        "        f\"Context (optional - use to enrich content):\\n{abstracts}\\n\\n\"\n",
        "        \"Start your response directly with the 'Abstract' section.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    paper_text = get_ai_response(messages)\n",
        "    if not paper_text or paper_text.startswith(\"[AI ERROR]\"):\n",
        "        return f\"‚ùå Could not generate paper: {paper_text}\", None, None, \"\"\n",
        "\n",
        "    # Pre-process the raw paper_text to remove any leading conversational filler\n",
        "    # Look for the first actual section heading (e.g., \"Abstract\", \"Introduction\") and start from there.\n",
        "    first_heading_match = re.search(r'^\\s*(Abstract|Introduction|Literature Review|Related Work|Background|Methodology|Methods|Materials and Methods|Results|Results and Discussion|Discussion|Conclusion|Conclusions|References)\\s*[:\\n\\r\\-]{0,2}\\s*$', paper_text, re.IGNORECASE | re.MULTILINE)\n",
        "    if first_heading_match:\n",
        "        paper_text = paper_text[first_heading_match.start():]\n",
        "    else:\n",
        "        # If no heading found, it means the entire response is likely malformed or filler\n",
        "        return f\"‚ùå Generated paper did not contain expected section headings. Raw output: {paper_text}\", None, None, \"\"\n",
        "\n",
        "\n",
        "    progress(0.35, desc=\"Parsing sections...\")\n",
        "    sections = parse_paper_sections(paper_text)\n",
        "\n",
        "    # If sections are empty, request them individually using context and any available abstract snippets\n",
        "    progress(0.45, desc=\"Ensuring all sections present...\")\n",
        "    for name, content in sections.items():\n",
        "        if not content or content.strip() == \"\":\n",
        "            progress(0.45 + 0.1 * (list(sections.keys()).index(name) / len(sections)), desc=f\"Generating missing {name}...\")\n",
        "            # Build a targeted prompt to produce that section only\n",
        "            prompt = (\n",
        "                f\"Produce ONLY the '{name}' section for a research paper on '{topic}'. Keep academic tone. \"\n",
        "                \"If data/results are not available, create plausible placeholder results clearly labeled as synthetic. \"\n",
        "                \"Return only the section content (no extra headings or introductory phrases).\"\n",
        "            )\n",
        "            # Provide context from other sections and abstracts\n",
        "            context = \"\\n\\n\".join([f\"{k}:\\n{v}\" for k, v in sections.items() if v and k != name])\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are an academic writer producing missing sections. Respond ONLY with the content of the requested section.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nContext:\\n{context}\\n\\nAbstracts:{abstracts}\"}\n",
        "            ]\n",
        "            sec_text = get_ai_response(messages)\n",
        "            # fallback: short placeholder\n",
        "            if not sec_text or sec_text.startswith(\"[AI ERROR]\"):\n",
        "                sec_text = f\"[MISSING] {name} could not be generated automatically.\"\n",
        "            sections[name] = sec_text.strip() # Strip any leading/trailing whitespace or conversational bits\n",
        "            time.sleep(0.7)\n",
        "\n",
        "    progress(0.60, desc=\"Processing quality for sections...\")\n",
        "    processed_sections = {}\n",
        "    section_reports = {}\n",
        "    for idx, (section_name, section_content) in enumerate(sections.items()):\n",
        "        # Skip references from heavy processing\n",
        "        if section_name.lower() == \"references\" or not enable_quality_check:\n",
        "            processed_sections[section_name] = section_content\n",
        "            section_reports[section_name] = {\"ai_score\": 0, \"plagiarism_score\": 0, \"grammar_issues\": 0, \"status\": \"skipped\" if section_name.lower()==\"references\" else \"not-checked\"}\n",
        "            continue\n",
        "\n",
        "        progress(0.60 + 0.35 * (idx / max(1, len(sections)-1)), desc=f\"Processing {section_name}...\")\n",
        "        processed_content, report = process_section_quality(section_content, section_name)\n",
        "        processed_sections[section_name] = processed_content\n",
        "        section_reports[section_name] = report\n",
        "        # small pause to avoid rate limits\n",
        "        time.sleep(0.6)\n",
        "\n",
        "    # Reconstruct the paper in canonical order\n",
        "    # Filter out empty sections from the final displayed text, but keep them in the processed_sections dict\n",
        "    # so the DOCX/PDF generation correctly places headings for potentially empty sections.\n",
        "    final_paper_parts = []\n",
        "    for name in [\"Abstract\",\"Introduction\",\"Literature Review\",\"Methodology\",\"Results and Discussion\",\"Conclusion\",\"References\"]:\n",
        "        content = processed_sections.get(name, \"\")\n",
        "        final_paper_parts.append(f\"{name}\\n{content}\".strip()) # Ensure heading is always present even if content is empty\n",
        "\n",
        "    ordered_text = \"\\n\\n\".join(final_paper_parts)\n",
        "\n",
        "    # Create files\n",
        "    progress(0.95, desc=\"Generating files...\")\n",
        "    os.makedirs(\"generated_papers\", exist_ok=True)\n",
        "    safe_name = re.sub(r'[^\\w\\-\\. ]', '', topic)[:50]\n",
        "\n",
        "    # DOCX\n",
        "    docx_path = f\"generated_papers/{safe_name}.docx\"\n",
        "    doc = Document()\n",
        "    doc.add_heading(topic, level=1)\n",
        "    for name in [\"Abstract\",\"Introduction\",\"Literature Review\",\"Methodology\",\"Results and Discussion\",\"Conclusion\",\"References\"]:\n",
        "        content = processed_sections.get(name, \"\")\n",
        "        doc.add_heading(name, level=2) # Always add the heading\n",
        "        if content:\n",
        "            for para in re.split(r'\\n{1,}', content):\n",
        "                if para.strip():\n",
        "                    doc.add_paragraph(para.strip())\n",
        "        else:\n",
        "            doc.add_paragraph(\"[Section content not generated]\") # Placeholder for empty content\n",
        "    doc.save(docx_path)\n",
        "\n",
        "    # PDF (simple streaming, keeps headings)\n",
        "    pdf_path = f\"generated_papers/{safe_name}.pdf\"\n",
        "    c = canvas.Canvas(pdf_path, pagesize=letter)\n",
        "    textobject = c.beginText(50, 750)\n",
        "    textobject.setFont(\"Times-Roman\", 11)\n",
        "    for name in [\"Abstract\",\"Introduction\",\"Literature Review\",\"Methodology\",\"Results and Discussion\",\"Conclusion\",\"References\"]:\n",
        "        content = processed_sections.get(name, \"\")\n",
        "        textobject.textLine(name) # Always add the heading\n",
        "        if content:\n",
        "            for line in content.split(\"\\n\"):\n",
        "                # wrap lines\n",
        "                for chunk in [line[i:i+100] for i in range(0, len(line), 100)]:\n",
        "                    if textobject.getY() < 60:\n",
        "                        c.drawText(textobject)\n",
        "                        c.showPage()\n",
        "                        textobject = c.beginText(50, 750)\n",
        "                        textobject.setFont(\"Times-Roman\", 11)\n",
        "                    textobject.textLine(chunk)\n",
        "        else:\n",
        "            textobject.textLine(\"[Section content not generated]\") # Placeholder for empty content\n",
        "        textobject.textLine(\"\")  # blank line between sections\n",
        "    c.drawText(textobject)\n",
        "    c.save()\n",
        "\n",
        "    # Build quality report string\n",
        "    quality_report_text = \"\"\n",
        "    if enable_quality_check:\n",
        "        quality_report_text = \"üìä QUALITY CHECK REPORT\\n\" + \"=\"*60 + \"\\n\\n\"\n",
        "        for section, report in section_reports.items():\n",
        "            quality_report_text += f\"üìÑ {section}:\\n\"\n",
        "            quality_report_text += f\"   AI Score: {report['ai_score']:.1f}%\\n\"\n",
        "            quality_report_text += f\"   Plagiarism: {report['plagiarism_score']:.1f}%\\n\"\n",
        "            quality_report_text += f\"   Grammar Issues: {report['grammar_issues']}\\n\"\n",
        "            quality_report_text += f\"   Status: {report['status']}\\n\\n\"\n",
        "\n",
        "    progress(1.0, desc=\"Complete!\")\n",
        "    return ordered_text, docx_path, pdf_path, quality_report_text\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Analytics Functions (keeping existing ones)\n",
        "# ------------------------------\n",
        "def get_session_statistics():\n",
        "    total_docs = len(session_docs)\n",
        "    total_messages = len(session_messages)\n",
        "    user_messages = len([m for m in session_messages if m[\"role\"]==\"user\"])\n",
        "    ai_messages = len([m for m in session_messages if m[\"role\"]==\"assistant\"])\n",
        "    total_chars = sum(len(doc[\"text\"]) for doc in session_docs)\n",
        "    avg_doc_length = total_chars / total_docs if total_docs > 0 else 0\n",
        "    pdf_count = len([d for d in session_docs if d.get(\"source\") == \"pdf\"])\n",
        "    arxiv_count = len([d for d in session_docs if d.get(\"source\") == \"arxiv\"])\n",
        "\n",
        "    return {\n",
        "        \"total_documents\": total_docs,\n",
        "        \"pdf_documents\": pdf_count,\n",
        "        \"arxiv_documents\": arxiv_count,\n",
        "        \"total_messages\": total_messages,\n",
        "        \"user_messages\": user_messages,\n",
        "        \"ai_messages\": ai_messages,\n",
        "        \"total_characters\": total_chars,\n",
        "        \"avg_doc_length\": avg_doc_length\n",
        "    }\n",
        "\n",
        "def get_top_keywords(top_n=15):\n",
        "    all_text = \" \".join([doc[\"text\"] for doc in session_docs])\n",
        "    words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
        "    stopwords = set([\"the\",\"and\",\"for\",\"with\",\"that\",\"this\",\"from\",\"are\",\"was\",\"were\",\"have\",\"has\",\"using\",\"our\",\"can\",\"which\",\"these\",\"their\",\"been\",\"into\",\"than\",\"more\",\"also\",\"will\",\"such\",\"when\",\"there\",\"other\",\"through\",\"about\",\"some\",\"only\",\"would\",\"between\"])\n",
        "    filtered_words = [w for w in words if w not in stopwords and len(w)>3]\n",
        "    word_counts = Counter(filtered_words)\n",
        "    return word_counts.most_common(top_n)\n",
        "\n",
        "def plot_keyword_bar():\n",
        "    keywords = get_top_keywords()\n",
        "    if not keywords:\n",
        "        fig = go.Figure()\n",
        "        fig.update_layout(title=\"Top Keywords\", plot_bgcolor=\"white\")\n",
        "        return fig\n",
        "    df = pd.DataFrame(keywords, columns=[\"Keyword\",\"Count\"])\n",
        "    fig = px.bar(df, x=\"Keyword\", y=\"Count\", title=\"Top Keywords\", color=\"Count\")\n",
        "    return fig\n",
        "\n",
        "def generate_insights_report_session():\n",
        "    stats = get_session_statistics()\n",
        "    keywords = get_top_keywords(10)\n",
        "\n",
        "    topic_info = f\"\\nüéØ Topic: {current_research_topic}\" if current_research_topic else \"\"\n",
        "    source_info = f\"\\nüìÑ Source: {'PDFs' if has_uploaded_pdfs else 'arXiv'}\"\n",
        "\n",
        "    report = f\"\"\"\n",
        "üìä Session Insights\n",
        "{'='*60}\n",
        "{topic_info}{source_info}\n",
        "\n",
        "üìö Documents: {stats['total_documents']} ({stats['pdf_documents']} PDFs, {stats['arxiv_documents']} arXiv)\n",
        "üí¨ Interactions: {stats['total_messages']}\n",
        "üìù Characters: {stats['total_characters']:,}\n",
        "\n",
        "üîë Keywords: {', '.join([k for k,c in keywords[:10]]) if keywords else 'None'}\n",
        "\"\"\"\n",
        "    return report\n",
        "\n",
        "def refresh_all_analytics():\n",
        "    report = generate_insights_report_session()\n",
        "    keyword_plot = plot_keyword_bar()\n",
        "    return report, keyword_plot\n",
        "\n",
        "# ------------------------------\n",
        "# Gradio UI\n",
        "# ------------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üß† ResearchMate AI ‚Äî Advanced Research Assistant with Quality Control\n",
        "    ### üìö Upload PDFs OR Set Topic | ‚úÖ AI Detection, Plagiarism Check, Humanization & Grammar Correction\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"üí¨ Chat Assistant\"):\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"üîÑ Clear Session\", variant=\"secondary\")\n",
        "\n",
        "            idea_input = gr.Textbox(label=\"üéØ Research Topic\", placeholder=\"e.g., Machine Learning in Healthcare\")\n",
        "            pdf_upload = gr.Files(label=\"üìÑ Upload PDFs (Optional)\", file_types=[\".pdf\"])\n",
        "            pdf_status = gr.Textbox(label=\"üìã Status\", interactive=False, lines=3)\n",
        "            chat_input = gr.Textbox(label=\"‚ùì Your Question\", placeholder=\"Ask anything...\", lines=2)\n",
        "            output_box = gr.Textbox(label=\"ü§ñ AI Answer\", interactive=False, lines=12)\n",
        "            btn = gr.Button(\"üöÄ Send\", variant=\"primary\")\n",
        "\n",
        "            def clear_session():\n",
        "                global session_messages, session_docs, current_research_topic, has_uploaded_pdfs\n",
        "                session_messages, session_docs = [], []\n",
        "                current_research_topic, has_uploaded_pdfs = None, False\n",
        "                try:\n",
        "                    collection.delete(where={})\n",
        "                except:\n",
        "                    pass\n",
        "                return \"‚úÖ Session cleared!\", \"\"\n",
        "\n",
        "            clear_btn.click(clear_session, outputs=[pdf_status, output_box])\n",
        "\n",
        "            def upload_handler(idea_topic, pdf_files):\n",
        "                global current_research_topic, has_uploaded_pdfs\n",
        "                if idea_topic:\n",
        "                    current_research_topic = idea_topic\n",
        "                if not pdf_files:\n",
        "                    return f\"‚úÖ Topic set: '{current_research_topic}'\" if current_research_topic else \"‚ö†Ô∏è Set topic or upload PDFs\"\n",
        "                try:\n",
        "                    pdf_texts, pdf_names = extract_text_from_pdfs(pdf_files)\n",
        "                    if pdf_texts:\n",
        "                        add_texts_to_rag(pdf_texts, pdf_names, source_type=\"pdf\")\n",
        "                        has_uploaded_pdfs = True\n",
        "                        return f\"‚úÖ Processed {len(pdf_texts)} PDF(s)\"\n",
        "                    return \"‚ùå Failed to extract text\"\n",
        "                except Exception as e:\n",
        "                    return f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "            pdf_upload.change(upload_handler, inputs=[idea_input, pdf_upload], outputs=pdf_status)\n",
        "            btn.click(combined_chat, inputs=[idea_input, pdf_upload, chat_input], outputs=output_box)\n",
        "\n",
        "        with gr.Tab(\"üìù Paper Generator with Quality Control\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            Generate research papers with automatic quality checking:\n",
        "            - ‚úÖ AI Content Detection\n",
        "            - ‚úÖ Plagiarism Checking\n",
        "            - ‚úÖ Text Humanization\n",
        "            - ‚úÖ Grammar Correction\n",
        "            \"\"\")\n",
        "\n",
        "            topic_box = gr.Textbox(label=\"Research Topic\", placeholder=\"e.g., Quantum Computing in Cryptography\")\n",
        "            quality_checkbox = gr.Checkbox(label=\"Enable Quality Processing (AI Detection, Plagiarism, Humanization, Grammar)\", value=True)\n",
        "\n",
        "            paper_output = gr.Textbox(label=\"Generated Paper\", lines=20, interactive=False)\n",
        "            quality_report_box = gr.Textbox(label=\"üìä Quality Report\", lines=10, interactive=False)\n",
        "\n",
        "            with gr.Row():\n",
        "                docx_download = gr.File(label=\"üì• Download DOCX\")\n",
        "                pdf_download = gr.File(label=\"üì• Download PDF\")\n",
        "\n",
        "            gen_btn = gr.Button(\"‚úçÔ∏è Generate Paper\", variant=\"primary\")\n",
        "\n",
        "            gen_btn.click(\n",
        "                generate_paper_with_quality_check,\n",
        "                inputs=[topic_box, quality_checkbox],\n",
        "                outputs=[paper_output, docx_download, pdf_download, quality_report_box]\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"üìä Analytics\"):\n",
        "            refresh_btn = gr.Button(\"üîÑ Refresh\", variant=\"primary\")\n",
        "            insights_text = gr.Textbox(label=\"Overview\", lines=15, interactive=False)\n",
        "            keyword_plot_box = gr.Plot(label=\"Keywords\")\n",
        "\n",
        "            refresh_btn.click(refresh_all_analytics, outputs=[insights_text, keyword_plot_box])\n",
        "            demo.load(refresh_all_analytics, outputs=[insights_text, keyword_plot_box])\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ]
}